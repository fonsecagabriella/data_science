{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B Test Process\n",
    "\n",
    "<img src=\"../img/a_b_process.png\">\n",
    "\n",
    "\n",
    "## **01. COMING UP WITH THE HYPOTHESIS**\n",
    "\n",
    "including how you measure the sucess (primary metric)\n",
    "\n",
    "_ie: Changing the \"Sign Up button\" to green will increase the sign-ups_\n",
    "\n",
    "ASK YOURSELF: If the chosen metric were to increase while everything stays the same, would you achieve your goal and address the problem?\n",
    "\n",
    "**- Statistical Hypothesis** \n",
    "- Null hypothesis (H0): what we want to reject\n",
    "- Acceptance hypothesis (H1): what we are subject to accept\n",
    "\n",
    "_ie:_\n",
    "_H0: The CRT of blue button is the same as the red button._\n",
    "_H1: CTR for red button is higher than the blue button_ \n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## **02. DESIGN THE TEST**\n",
    "\n",
    "### **02.1 POWER ANALYSIS**\n",
    "\n",
    "<img src=\"../img/power_analysis.png\">\n",
    "\n",
    "**Determine the POWER (1 - beta, where beta = probability of type 2 error) of the test**\n",
    "_Probability of making  type 2 error (false negative)_\n",
    "Usually used 80%\n",
    "\n",
    "**Determine the SIGNIFICANCE (alpha) level of the test:**\n",
    "_Probability of main type 1 error (false positive)_\n",
    "Generally used 5%\n",
    "\n",
    "**Determine MINIMUM DETECTABLE EFFECT (delta) of the test**\n",
    "It ensures that your test is sensitive enough to detect meaningful differences, but not so sensitive that it detects trivial changes that aren't practically significant.\n",
    "\n",
    "_ie. If we are looking at CTR, and we have a MDE of 5%. If the difference in CTR between the two versions is smaller than the MDE (5%), than the test might not be able to confidently say which version is better._\n",
    "\n",
    "If the MDE is too high, you might need a larger sample size.\n",
    "Likewise, if you\n",
    "\n",
    "\n",
    "_Suppose your baseline conversion rate is 10%. You want to detect an increase to at least 12% (a 2% absolute increase). You set your power at 80% and alpha at 5%. With a sample size of 1,000 per group (control and test), you input these values into an MDE calculator, and it tells you the minimum effect you can reliably detect given these parameters._\n",
    "\n",
    "\n",
    "### **02.2 CALCULATE MIN SAMPLE SIZE**\n",
    "There are different approaches for binary metrics (ie. user clicked or not clicked), and continuos metrics (ie. a percentage value of clicks.)\n",
    "\n",
    "\n",
    "For the example below we will generate some data to play with.\n",
    "\n",
    "Below how you can calculate sample size with Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required sample size per group: 251\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.power import NormalIndPower\n",
    "\n",
    "# Parameters\n",
    "baseline_rate = 0.20  # Baseline conversion rate\n",
    "mde = 0.05  # Minimum Detectable Effect that you want\n",
    "effect_size = mde / baseline_rate  # Relative effect size\n",
    "power = 0.80  # Desired power\n",
    "alpha = 0.05  # Significance level\n",
    "\n",
    "# Create an instance of NormalIndPower\n",
    "power_analysis = NormalIndPower()\n",
    "\n",
    "# Calculate sample size\n",
    "sample_size = power_analysis.solve_power(effect_size=effect_size,\n",
    "                                         power=power,\n",
    "                                         alpha=alpha,\n",
    "                                         ratio=1)  # Ratio of the sample sizes in the two groups\n",
    "\n",
    "print(f\"Required sample size per group: {sample_size:.0f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **02.3 TEST DURATION**\n",
    "\n",
    "**Duration = N / # visitors per day**\n",
    "where the N is the size of the Min Sample Size\n",
    "\n",
    "NOTES:\n",
    "- Try to run the test in the times that make sense to your test (consider external periods, seasonality, etc.)\n",
    "- NEVER STOP THE TEST WHEN YOU ACHIEVE SIGNIFICANTE LEVEL, this is called P-hacking. Determine before-hand the number of days you should run the test before\n",
    "- NOVELTY EFFECT: When the period if too short, users then to react to it, but the effect can \"wear off during time\".\n",
    "- MATURATION EFFECT: when you run the test for too long, you risk that external effects have an influence in your results.\n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "## **03. RUN THE TEST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_clicked</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ctr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>ctr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>ctr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>ctr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>ctr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_clicked group\n",
       "0           0   ctr\n",
       "1           0   ctr\n",
       "2           1   ctr\n",
       "3           1   ctr\n",
       "4           0   ctr"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "N_samples_experiment = 1000\n",
    "N_samples_control = 1000\n",
    "\n",
    "# Generate random click data, normal distribution\n",
    "click_experiment = pd.Series(np.random.binomial(1, 0.5, size=N_samples_experiment))\n",
    "click_control = pd.Series(np.random.binomial(1, 0.2, size=N_samples_control))\n",
    "\n",
    "\n",
    "# Simulated example data based on required sample size\n",
    "# For simplicity, let's assume the experiment resulted in the following observed counts\n",
    "#click_control = np.random.binomial(n=sample_size, p=baseline_rate)\n",
    "#click_experiment = np.random.binomial(n=sample_size, p=baseline_rate + mde)\n",
    "\n",
    "\n",
    "\n",
    "# Generate group identifier\n",
    "exp_id = pd.Series(np.repeat(\"exp\", N_samples_experiment))\n",
    "ctr_id = pd.Series(np.repeat(\"ctr\", N_samples_control))\n",
    "\n",
    "df_exp = pd.concat([click_experiment, exp_id], axis=1)\n",
    "df_ctr = pd.concat([click_control, ctr_id], axis=1)\n",
    "\n",
    "df_exp.columns = [\"is_clicked\", \"group\"]\n",
    "df_ctr.columns = [\"is_clicked\", \"group\"]\n",
    "\n",
    "df_ab_test = pd.concat([df_ctr, df_exp])\n",
    "\n",
    "df_ab_test.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **04. ANALYSE RESULTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clicks by group \n",
      " {(0, 'ctr'): 795, (1, 'exp'): 505, (0, 'exp'): 495, (1, 'ctr'): 205}\n",
      "\n",
      "\n",
      "Click probability in Control 0.205\n",
      "Click probability in Experiment 0.505\n"
     ]
    }
   ],
   "source": [
    "# Shows the numbers of clicks and non-clicks by the group\n",
    "df_ab_test_dic = df_ab_test.value_counts().to_dict()\n",
    "print(f\"Clicks by group \\n {df_ab_test_dic}\\n\\n\")\n",
    "\n",
    "# Calculates the probabilities of clicks for both groups\n",
    "p_hat_exp = df_ab_test_dic[(1, \"exp\")] / N_samples_experiment\n",
    "p_hat_ctr = df_ab_test_dic[(1, \"ctr\")] / N_samples_control\n",
    "\n",
    "print(f\"Click probability in Control {p_hat_ctr}\")\n",
    "print(f\"Click probability in Experiment {p_hat_exp}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi2 Statistic: 195.2200021836445\n",
      "P-Value: 2.3067270314680285e-44\n",
      "Reject the null hypothesis - there is a significant difference between the groups.\n",
      "Fisher's Exact Test P-Value: 2.2579432263396825e-45\n",
      "Reject the null hypothesis - there is a significant difference between the groups.\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import chi2_contingency, fisher_exact\n",
    "\n",
    "# create the contigency table\n",
    "data = np.array([[df_ab_test_dic[(1, \"ctr\")], df_ab_test_dic[(0, \"ctr\")]], #control group\n",
    "[df_ab_test_dic[(1, \"exp\")], df_ab_test_dic[(0, \"exp\")]]]) # experimental group\n",
    "\n",
    "# Perform the chi-square test\n",
    "chi2, p, dof, expected = chi2_contingency(data)\n",
    "\n",
    "print(f\"Chi2 Statistic: {chi2}\")\n",
    "print(f\"P-Value: {p}\")\n",
    "\n",
    "# Interpret the result\n",
    "alpha = 0.05  # significance level\n",
    "if p < alpha:\n",
    "    print(\"Reject the null hypothesis - there is a significant difference between the groups.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis - there is no significant difference between the groups.\")\n",
    "\n",
    "# If sample sizes are small or any expected frequency is below 5, use Fisher's Exact Test\n",
    "oddsratio, p_value = fisher_exact(data)\n",
    "print(f\"Fisher's Exact Test P-Value: {p_value}\")\n",
    "\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis - there is a significant difference between the groups.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis - there is no significant difference between the groups.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__REFERENCES__\n",
    "\n",
    "[https://tatevaslanyan.com/complete-guide-to-a-b-testing-design-implementation-and-pitfalls/]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
