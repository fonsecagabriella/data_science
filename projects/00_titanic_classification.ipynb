{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_WORKING I(N) PROGRESS_\n",
    "# PREDICTING SURVIVORS - TITANIC\n",
    "\n",
    "The \"Titanic Project\" is a common project among people who are starting in Machine Learning.\n",
    "The goal of the project is to predict is a passenger survived.\n",
    "\n",
    "The dataset was downloaded from Kaggle. https://www.kaggle.com/code/alexisbcook/titanic-tutorial\n",
    "It has 418 rows and 12 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential libraries for data manipulation\n",
    "import numpy as np #linear algebra\n",
    "import pandas as pd # data processing\n",
    "import matplotlib.pyplot as plt # graph plots\n",
    "\n",
    "\n",
    "import seaborn as sns # plots\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# basic styling for graphs\n",
    "plt.style.use(\"seaborn-dark\")\n",
    "font = {\"family\": \"Azeret Mono\",\n",
    "\"weight\": \"bold\",\n",
    "\"size\": 14}\n",
    "\n",
    "plt.rcParams.update({\"font.family\": font[\"family\"], \"font.weight\": font[\"weight\"], \"font.size\": font[\"size\"]})\n",
    "\n",
    "# loads data\n",
    "data_train = pd.read_csv(\"data/00_titanic/train.csv\")\n",
    "\n",
    "# prepare the test set\n",
    "data_test = pd.read_csv(\"data/00_titanic/test.csv\")\n",
    "data_test.info()\n",
    "\n",
    "# prepare path to save model results for comparision\n",
    "results = \"../output/titanic_classification.csv\"\n",
    "\n",
    "# take a glimpse at data\n",
    "data_train.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential question: Survival rate\n",
    "\n",
    "print(f\"Survival Rate {((data_train['Survived'] == 1).sum()/data_train['Survived'].count()):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"hey\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Data Cleaning\n",
    "\n",
    "Before I start playing, it's necessary to get my hands \"dirty\" by cleaning the data! üßª\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get info about data - check amount of null values\n",
    "print(\"Train set\")\n",
    "data_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test set\")\n",
    "data_test.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the number of empty values above, I decided to treat them as following:\n",
    "\n",
    "- Age: I will use an estimator, the median of ages.\n",
    "- Cabin: There are a lot of missing values, but the existing ones could tell me something. As I don't know it yet, I will replaced the null values with \"UNK\", for *unknow*. \n",
    "- Embarked: Since we have only two rows with null values in the train set, it's unlikely these rows will make a large difference in the final model. So I will drop them.\n",
    "- Fare: The test set has one null value for fare. I will replace it by the mean of fares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# another way to write the code below\n",
    "# data_train.Age = data_train.Age.fillna(data_train.Age.median())\n",
    "\n",
    "# first treat the train data\n",
    "data_train[\"Age\"] = data_train[\"Age\"].fillna(data_train[\"Age\"].median()) # Age\n",
    "data_train.dropna(subset=[\"Embarked\"], inplace=True) # Embarked\n",
    "data_train[\"Cabin\"] = data_train[\"Cabin\"].fillna(\"UNK\") # Cabin\n",
    "\n",
    "# now treat the test data\n",
    "data_test[\"Age\"] = data_test[\"Age\"].fillna(data_test[\"Age\"].median()) # Age\n",
    "data_test[\"Fare\"] = data_test[\"Fare\"].fillna(data_test[\"Fare\"].median()) #Fare\n",
    "data_test[\"Cabin\"] = data_test[\"Cabin\"].fillna(\"UNK\")  # Cabin\n",
    "# we don't need to treat 'Emabarked' since there are no null values\n",
    "\n",
    "\n",
    "# from now, I will create the X_train and X_test, removing the features I won't need from this step on\n",
    "# For example, \"the ticket number\" can be removed\n",
    "# Name will be removed after the data engineering step\n",
    "# Survived will be removed once we start the models, when we set y_train and y_test\n",
    "features = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\", \"Name\", \"Cabin\"]\n",
    "\n",
    "X_train = data_train[features].copy()\n",
    "X_test = data_test[features].copy()\n",
    "\n",
    "# Adds \"survivor to list of features\"\n",
    "# this will be removed after the data visualisation\n",
    "X_train[\"Survived\"] = data_train[\"Survived\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just checking\n",
    "X_train.info()\n",
    "\n",
    "X_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the data above, I can set my features (X arrays) and y (label for training). While the label will be logically column \"Survived\", for the choosen features are less obvious. Nevertheless some features are likely to bring little information in future predictions, such as the \"name\" the passenger, their ticket number, since they are unique. I will move on inspecting those cases. But first, let me see if I can create some new features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Feature engineering\n",
    "\n",
    "Now I will explore if some features could be created based based on my  interpreation of the situation. I find this to be an exciting step! You need to combine creativity with logical reasoning, common sense, and the subject knowledge.\n",
    "\n",
    "This is a fairly simple dataset. After exploring the rows, I decided to create two new columns.\n",
    "\n",
    "- Family Fame: We can potentially argue that the family name has an influence on the survival chance of a person. If a family was considered prestigious, they could be more likely to survive.\n",
    "- Number of family members: This was suggested by ChatGPT. I don't have any issues in admiting AI Agents are my friends during working from home (among my three cats üê± üê± üê±)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # regular expressions\n",
    "\n",
    "# family members\n",
    "# Add new feature using .loc to avoid SettingWithCopyWarning\n",
    "X_train.loc[:, \"N_Family_Members\"] = X_train[\"SibSp\"] + X_train[\"Parch\"]\n",
    "X_test.loc[:, \"N_Family_Members\"] = X_test[\"SibSp\"] + X_test[\"Parch\"]\n",
    "\n",
    "\n",
    "# family name\n",
    "# the condition is treated with regex\n",
    "# \"the first words that appear up the comma, in \"Name\" \n",
    "def extract_family_name(name):\n",
    "    match = re.match(r'^([^,]+)', name)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return \"Uknown\"\n",
    "\n",
    "# extract the family name and create a new column\n",
    "X_train[\"Family_Name\"] = X_train[\"Name\"].apply(extract_family_name)\n",
    "X_test[\"Family_Name\"] = X_test[\"Name\"].apply(extract_family_name)\n",
    "\n",
    "\n",
    "\n",
    "# reviews the training set, to check if everything looks good\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many groups were formed from \"Family_Name\"\n",
    "print(X_train[\"Family_Name\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just checking... \n",
    "X_train.info()\n",
    "\n",
    "X_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data Exploration\n",
    "\n",
    "Some data vis to get familiar with the data. Supported by the graph below, the data tells us that:\n",
    "\n",
    "- The data is normally distributed for Age, generally and considering the Survivors/Non-survivors\n",
    "- The survival rate among individuals with the Sex as \"Female\" is higher than among individuals with the Sex as \"Male\".\n",
    "- There is an increase in survival rate according to higher classes. Higher classes have a higher survival rate, while lower classes have a higher rate of non-survivors.\n",
    "- Passengers of the higher classes have paid more for their fairs.\n",
    "- **The P-class has the highest degree of association with the survival rate.**\n",
    "- **Men died more than women**.The percentage of man who didn't survived is higher than the woman who survived.\n",
    "- Based on the visualisations the features Gender, PClass and Fair seem to have a higher correlation with the survival rate. When I create the models, I will also test one with these \"special_features\" to compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survival rate by Gender\n",
    "sns.countplot(x=X_train[\"Sex\"], hue= X_train[ \"Survived\"], palette={0: \"#ffc1d6\", 1: \"#1b48ab\"})\n",
    "plt.xlabel(\"Sex\", labelpad=20)\n",
    "plt.ylabel(\"Number of Passengers\", labelpad=20)\n",
    "plt.title(\"TITANICS SURVIVAL RATE BY SEX\", pad=40, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survival rate by Class\n",
    "# Is the certain class more likely to survive than others?\n",
    "\n",
    "sns.countplot(x=\"Pclass\", hue=\"Survived\", data=data_train, palette={0: \"#ffc1d6\", 1: \"#1b48ab\"})\n",
    "\n",
    "plt.title(\"Survival Count by Passenger Class\")\n",
    "plt.xlabel(\"Passenger Class\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend([\"Not Survived\", \"Survived\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survival rate by Cabin\n",
    "X_train.Cabin.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age Distribution\n",
    "# Distribution plot of ages\n",
    "sns.histplot(X_train['Age'].dropna(), bins=30, kde=True)\n",
    "plt.title('Age Distribution')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Filter data for survivors and non-survivors\n",
    "survived_data = X_train[X_train['Survived'] == 1]['Age']\n",
    "not_survived_data = X_train[X_train['Survived'] == 0]['Age']\n",
    "\n",
    "# Create distribution plots for survivors and non-survivors\n",
    "sns.histplot(survived_data, bins=30, kde=True, color='#1b48ab', label='Survived')\n",
    "sns.histplot(not_survived_data, bins=30, kde=True, color='#ffc1d6', label='Not Survived')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Age Distribution among Survivors and Non-Survivors')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Survival rate by age\n",
    "# Violin plot of age by survival\n",
    "sns.violinplot(x='Survived', y='Age', data=X_train, split=True)\n",
    "plt.title('Survival by Age')\n",
    "plt.xlabel('Survived')\n",
    "plt.ylabel('Age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fare distribution by Class\n",
    "# Box plot of fare by passenger class\n",
    "sns.boxplot(x='Pclass', y='Fare', hue=\"Survived\", data=X_train, palette={0: \"#ffc1d6\", 1: \"#1b48ab\"})\n",
    "plt.title('Fare Distribution by Passenger Class', pad=40)\n",
    "plt.xlabel('Passenger Class')\n",
    "plt.ylabel('Fare')\n",
    "plt.show()\n",
    "\n",
    "# Survival Rate by Embarkation Port\n",
    "# Count plot of survival by embarkation port\n",
    "sns.countplot(x='Embarked', hue='Survived', data=X_train, palette={0: \"#ffc1d6\", 1: \"#1b48ab\"})\n",
    "plt.title('Survival Count by Embarkation Port', pad=40)\n",
    "plt.xlabel('Embarkation Port')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(['Not Survived', 'Survived'])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Pairplot of Features\n",
    "# Pairplot of selected features\n",
    "sns.pairplot(X_train[['Age', 'Fare', 'Pclass', 'Survived']], hue='Survived', palette={0: \"#ffc1d6\", 1: \"#1b48ab\"})\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Family Size Distribution\n",
    "sns.histplot(X_train['N_Family_Members'], bins=10, kde=True)\n",
    "plt.title('Family Size Distribution', pad=40)\n",
    "plt.xlabel('Family Size')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Sib Distribution\n",
    "sns.histplot(X_train['SibSp'], bins=10, kde=True)\n",
    "plt.title('Siblings and Spouses Distribution', pad=40)\n",
    "plt.xlabel('Number of siblings / spouses on board')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Parch distribution\n",
    "sns.histplot(X_train['Parch'], kde=True)\n",
    "plt.title(\"Parents and children Distribution\", pad=40)\n",
    "plt.xlabel(\"Number of parents/children on board\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Correlation Heatmap\n",
    "\n",
    "# Drop non-numeric columns\n",
    "numeric_columns = X_train.select_dtypes(include=['number']).columns\n",
    "X_train_numeric = X_train[numeric_columns]\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = X_train_numeric.corr()\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Heatmap', pad=40)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# plot overview based on age and fair\n",
    "sns.scatterplot(y=data_train[\"Fare\"], x=data_train[\"Age\"], hue=data_train[\"Survived\"], palette={0: \"#ffc1d6\", 1: \"#1b48ab\"})\n",
    "\n",
    "plt.xlabel(\"Age\", labelpad=20)\n",
    "plt.ylabel(\"Fare\", labelpad=20)\n",
    "plt.title(\"Overview of Titanic Survivors based on Age and Fair\", pad=40)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with the swarm plot we can potentially see some clusters in the data\n",
    "sns.swarmplot(x=\"Survived\", y= \"Age\", hue=\"Sex\", data=data_train, palette={\"female\": \"#a2a0e7\", \"male\": \"#1b48ab\"})\n",
    "plt.title(\"Sex Distribution and Survival\", pad=40)\n",
    "plt.show()\n",
    "\n",
    "sns.catplot(x = \"Sex\",y = \"Survived\",hue = \"Sex\", kind = \"bar\", data = data_train,\n",
    "palette={\"female\": \"#a2a0e7\", \"male\": \"#1b48ab\", })\n",
    "# Breakdown by gender within Survived category, )\n",
    "plt.title(\"Percentage of Survival Among Females and Males\", pad=40)\n",
    "plt.show()\n",
    "\n",
    "# class distribition and survival\n",
    "sns.swarmplot(x=\"Survived\", y= \"Age\", hue=\"Pclass\", data=data_train)\n",
    "plt.title(\"Class Distribution and Survival\", pad=40)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 (Pre) Data Encoding\n",
    "\n",
    "Next, I will move on by encoding the categorical variables. Before follow with the enconding, it is good to check if the data is consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if data is consistent for all columns with object type - what will be enconded\n",
    "print(X_train.info())\n",
    "print(f\"\\n\\n{X_train.Sex.value_counts()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.Embarked.value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.Family_Name.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Family_Name\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 *Machine Learning Models* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "# evaluation metrics\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, PowerTransformer\n",
    "\n",
    "\n",
    "# Define the OneHotEncoder for categorical features\n",
    "one_hot_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "\n",
    "# create the labels\n",
    "y_train = X_train[\"Survived\"]\n",
    "\n",
    "# select features\n",
    "#features = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Embarked\", \"Cabin\", \"N_Family_Members\", \"Family_Name\", \"Fare\"]\n",
    "features = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"Parch\"]\n",
    "\n",
    "\n",
    "\n",
    "# create a subset of X_train with the features available to use\n",
    "X_train_selected = X_train[features]\n",
    "X_test_selected = X_test[features]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded\n",
    "#X_train_encoded = pd.get_dummies(X_train_selected)\n",
    "#X_test_encoded = pd.get_dummies(X_test_selected)\n",
    "\n",
    "# create test & validation+train\n",
    "X_trainval_model,  X_test_model, y_trainval_model, y_test_model = train_test_split(X_train_selected, y_train, test_size=0.1, random_state=25)\n",
    "\n",
    "# create validation and train sets\n",
    "X_train_model, X_validation, y_train_model, y_validation = train_test_split(X_trainval_model, y_trainval_model, test_size=0.2, random_state=25)\n",
    "\n",
    "print(f\"Test Set Size: {X_test_model.shape}\")\n",
    "print(f\"Train Set Size: {X_train_model.shape}\")\n",
    "print(f\"Validation Set Size: {X_validation.shape}\")\n",
    "\n",
    "all_models = []\n",
    "\n",
    "\n",
    "\n",
    "X_train_model.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier with Grid Search\n",
    "\n",
    "# define parameter grid\n",
    "parameter_grid = {\n",
    "    \"randomforestclassifier__n_estimators\": [50, 100, 200, 300, 400, 500], # this is the number of trees\n",
    "    \"randomforestclassifier__max_depth\": [None, 5, 10, 20, 30, 40, 50],\n",
    "    \"randomforestclassifier__min_samples_split\": [2, 5, 10],\n",
    "    \"randomforestclassifier__min_samples_leaf\": [1, 2, 4],\n",
    "    \"randomforestclassifier__max_features\": [\"auto\", \"sqtr\", \"log2\"],\n",
    "    \"randomforestclassifier__bootstrap\": [True, False]\n",
    "\n",
    "}\n",
    "\n",
    "# initialise the random forest\n",
    "# m_forest = RandomForestClassifier(random_state=25)\n",
    "\n",
    "pipe = make_pipeline(RandomForestClassifier(random_state=25))\n",
    "\n",
    "grid_random_forest = GridSearchCV(pipe, parameter_grid, cv=5)\n",
    "\n",
    "grid_random_forest.fit(X_train_model, y_train_model)\n",
    "\n",
    "# Make predictions and get probabilities\n",
    "y_pred = grid_random_forest.predict(X_test_model)\n",
    "y_proba = grid_random_forest.predict_proba(X_test_model)[:, 1]  # Probability of survived class\n",
    "\n",
    "# Calculate AUC score\n",
    "auc = roc_auc_score(y_test_model, y_proba)\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test_model, y_pred)\n",
    "precision = precision_score(y_test_model, y_pred)\n",
    "recall = recall_score(y_test_model, y_pred)\n",
    "\n",
    "# save results\n",
    "model_results = {\"model\": \"Random Forest\",\n",
    "                    \"parameters\":grid_random_forest.best_params_,\n",
    "                    \"best cv score\": grid_random_forest.best_score_,\n",
    "                    \"validation score\": grid_random_forest.score(X_validation, y_validation),\n",
    "                    \"test score\": grid_random_forest.score(X_test_model, y_test_model),\n",
    "                    \"AUC\": auc,\n",
    "                    \"Accuracy\": accuracy,\n",
    "                    \"Precision\": precision,\n",
    "                    \"Recall\": recall }\n",
    "\n",
    "all_models.append(model_results)\n",
    "\n",
    "# print results\n",
    "for key, value in model_results.items():\n",
    "    print(f\"{key} : {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Classifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Define the ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Apply OneHotEncoder to categorical columns\n",
    "        (\"cat\", one_hot_encoder, [\"Sex\", \"Embarked\"]),\n",
    "        # Apply PowerTransformer to left-skewed columns\n",
    "        (\"skewed\", PowerTransformer(), [\"Fare\", \"Parch\"]),\n",
    "        # Leave \"Age\" out for now; it will be handled in the pipeline\n",
    "        (\"identity\", \"passthrough\", [\"Age\"])\n",
    "    ],\n",
    "    remainder='drop'  # Drop any other columns not specified\n",
    ")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    ('age_scaler', StandardScaler()),  # Name the step 'scaler'\n",
    "    ('classifier', GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "\n",
    "parameter_grid = [\n",
    "{\"classifier\": [GradientBoostingClassifier()],\n",
    "\"age_scaler\":[StandardScaler(), MinMaxScaler()],\n",
    "'classifier__n_estimators': [100, 200, 300], #number of trees\n",
    "'classifier__learning_rate': [0.001,0.005,0.01],\n",
    "'classifier__max_depth': [5, 8,10],\n",
    "'classifier__min_samples_split': [2, 5, 10],\n",
    "'classifier__min_samples_leaf': [4, 6, 8],\n",
    "'classifier__subsample': [0.2,0.5, 0.8],\n",
    "\"classifier__max_features\": ['auto', 'sqrt', 'log2', None] }\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "stratified_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "grid_gbc = GridSearchCV(pipe, parameter_grid, cv=stratified_cv, n_jobs=-1)\n",
    "grid_gbc.fit(X_train_model, y_train_model)\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions and get probabilities\n",
    "y_pred = grid_gbc.predict(X_test_model)\n",
    "y_proba = grid_gbc.predict_proba(X_test_model)[:, 1]  # Probability of survived class\n",
    "\n",
    "# Calculate AUC score\n",
    "auc = roc_auc_score(y_test_model, y_proba)\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test_model, y_pred)\n",
    "precision = precision_score(y_test_model, y_pred)\n",
    "recall = recall_score(y_test_model, y_pred)\n",
    "\n",
    "# save results\n",
    "model_results = {\"model\": \"Gradient Boosting Classifier\",\n",
    "                    \"parameters\":grid_gbc.best_params_,\n",
    "                    \"best cv score\": grid_gbc.best_score_,\n",
    "                    \"validation score\": grid_gbc.score(X_validation, y_validation),\n",
    "                    \"test score\": grid_gbc.score(X_test_model, y_test_model),\n",
    "                    \"AUC\": auc,\n",
    "                    \"Accuracy\": accuracy,\n",
    "                    \"Precision\": precision,\n",
    "                    \"Recall\": recall }\n",
    "\n",
    "all_models.append(model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC model\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()), # step 01: scale the data\n",
    "        (\"svc\", SVC(probability=True)) # apply svc\n",
    "    ]\n",
    ")\n",
    "\n",
    "parameter_grid = {\n",
    "    'svc__C': [0.01, 1, 100],        # Range of C values\n",
    "    'svc__gamma': [0.01 , 1, 100], # Range of gamma values\n",
    "    'svc__kernel': [\"linear\", \"rgb\"],    # Different kernels\n",
    "    \"svc__degree\": [2, 3] # for the poly variation\n",
    "}\n",
    "\n",
    "grid_svc = GridSearchCV(pipeline, parameter_grid, cv=5, scoring='accuracy', n_jobs=-3)\n",
    "grid_svc.fit(X_train_model, y_train_model)\n",
    "\n",
    "# Make predictions and get probabilities\n",
    "y_pred = grid_svc.predict(X_test_model)\n",
    "y_proba = grid_svc.predict_proba(X_test_model)[:, 1]  # Probability of survived class\n",
    "\n",
    "# Calculate AUC score\n",
    "auc = roc_auc_score(y_test_model, y_proba)\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test_model, y_pred)\n",
    "precision = precision_score(y_test_model, y_pred)\n",
    "recall = recall_score(y_test_model, y_pred)\n",
    "\n",
    "# save results\n",
    "model_results = {\"model\": \"Support Vector Machine\",\n",
    "                    \"parameters\":grid_svc.best_params_,\n",
    "                    \"best cv score\": grid_svc.best_score_,\n",
    "                    \"validation score\": grid_svc.score(X_validation, y_validation),\n",
    "                    \"test score\": grid_svc.score(X_test_model, y_test_model),\n",
    "                    \"AUC\": auc,\n",
    "                    \"Accuracy\": accuracy,\n",
    "                    \"Precision\": precision,\n",
    "                    \"Recall\": recall }\n",
    "\n",
    "all_models.append(model_results)\n",
    "\n",
    "# print results\n",
    "for key, value in model_results.items():\n",
    "    print(f\"{key} : {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler, PowerTransformer, PolynomialFeatures\n",
    "\n",
    "#pipe = make_pipeline((\"scaler\", StandardScaler()), #placeholder for the scaler\n",
    "#(\"logisticregression\", LogisticRegression(max_iter=1000)))\n",
    "\n",
    "# Define the OneHotEncoder for categorical features\n",
    "one_hot_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "\n",
    "# Define the ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Apply OneHotEncoder to categorical columns\n",
    "        (\"cat\", one_hot_encoder, [\"Sex\"]),\n",
    "        # Apply PowerTransformer to left-skewed columns\n",
    "        (\"skewed\", PowerTransformer(), [\"Fare\"]),\n",
    "        # Leave \"Age\" out for now; it will be handled in the pipeline\n",
    "        (\"identity\", \"passthrough\", [\"Age\"])\n",
    "    ],\n",
    "    remainder='drop'  # Drop any other columns not specified\n",
    ")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    ('age_scaler', StandardScaler()),  # Name the step 'scaler'\n",
    "    ('poly', PolynomialFeatures(include_bias=False)),  # Include PolynomialFeatures\n",
    "    ('logisticregression', LogisticRegression(max_iter=1000))  # Name the step 'logisticregression'\n",
    "])\n",
    "\n",
    "# for the parameter grid it is important to use the name given by 'make_pipeline'\n",
    "# you can see this with the function steps\n",
    "\n",
    "parameter_grid = {\n",
    "    \"age_scaler\": [StandardScaler(), MinMaxScaler()],\n",
    "    \"poly__degree\": [3, 5],  # Try degrees 1, 2, and 3 for polynomial features\n",
    "    \"logisticregression__C\": [0.01, 0.1, 1, 10, 100],\n",
    "    'logisticregression__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    'logisticregression__solver': ['liblinear', 'saga', 'lbfgs'],  # note: 'lbfgs' and 'liblinear' do not support 'elasticnet'\n",
    "    'logisticregression__max_iter': [100, 200, 500],\n",
    "    'logisticregression__fit_intercept': [True, False],\n",
    "    'logisticregression__class_weight': [None, 'balanced']\n",
    "    }\n",
    "\n",
    "grid_logreg = GridSearchCV(pipe, parameter_grid, cv=5)\n",
    "\n",
    "grid_logreg.fit(X_train_model, y_train_model)\n",
    "\n",
    "\n",
    "# Make predictions and get probabilities\n",
    "y_pred = grid_logreg.predict(X_test_model)\n",
    "y_proba = grid_logreg.predict_proba(X_test_model)[:, 1]  # Probability of survived class\n",
    "\n",
    "# Calculate AUC score\n",
    "auc = roc_auc_score(y_test_model, y_proba)\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test_model, y_pred)\n",
    "precision = precision_score(y_test_model, y_pred)\n",
    "recall = recall_score(y_test_model, y_pred)\n",
    "\n",
    "# save results\n",
    "model_results = {\"model\": \"Logistic Regression\",\n",
    "                    \"parameters\":grid_logreg.best_params_,\n",
    "                    \"best cv score\": grid_logreg.best_score_,\n",
    "                    \"validation score\": grid_logreg.score(X_validation, y_validation),\n",
    "                    \"test score\": grid_logreg.score(X_test_model, y_test_model),\n",
    "                    \"AUC\": auc,\n",
    "                    \"Accuracy\": accuracy,\n",
    "                    \"Precision\": precision,\n",
    "                    \"Recall\": recall }\n",
    "\n",
    "all_models.append(model_results)\n",
    "\n",
    "# print results\n",
    "for key, value in model_results.items():\n",
    "    print(f\"{key} : {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further exploration and questions\n",
    "\n",
    "- Does the features I created helped the model?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in all_models:\n",
    "    for i in model:\n",
    "        if i==\"model\":\n",
    "            print (model[i])\n",
    "        if i != \"parameters\" and i!=\"model\":\n",
    "            print(f\"{i} : {model[i]:.3f}\")\n",
    "\n",
    "    print(\"===\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = [all_models[i] for i in [0,1,2,4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I found the best parameters for several models, I will move one and check other measures of sucess, in order to decide which model to use in the end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Define voting scheme (hard or soft)\n",
    "voting = 'hard'  # Replace with 'soft' for soft voting\n",
    "\n",
    "# Create the VotingClassifier\n",
    "ensemble = VotingClassifier(estimators=[('Forest', grid_random_forest), ('LogReg', grid_logreg), (\"GBC\", grid_gbc)], voting=voting)\n",
    "ensemble.fit(X_train_model, y_train_model)\n",
    "\n",
    "print(ensemble.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = grid_logreg.predict(X_test_selected)\n",
    "\n",
    "\n",
    "output = pd.DataFrame({'PassengerId': data_test.PassengerId, 'Survived': predictions})\n",
    "output.to_csv('submission_grid_logreg_poly_features.csv', index=False)\n",
    "print(\"Your submission was successfully saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_logreg.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.concat([X_test, data_test[\"PassengerId\"]], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_encoded.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_logreg.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
