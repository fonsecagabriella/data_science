{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML ENGINEERING\n",
    "\n",
    "<img src=\"../img/ml-engineering.png\" width=\"60%\" >\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Static vs Dynamic Training\n",
    "- **A static model is trained offline**. That is, we train the model exactly once and then use that trained model for a while.\n",
    "- **A dynamic model is trained online**. That is, data is continually entering the system and we're incorporating that data into the model through continuous updates.\n",
    "\n",
    "## Static vs Dynamic Inference (making predictions)\n",
    "- **Offline inference**, meaning that you make all possible predictions in a batch, using a MapReduce or something similar. You then write the predictions to an SSTable or Bigtable, and then feed these to a cache/lookup table.\n",
    "- **Online inference**, meaning that you predict on demand, using a server.\n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "# **THE ML FINE PRINT**\n",
    "The following three basic assumptions guide generalization:\n",
    "\n",
    "- We draw examples **independently and identically** (i.i.d) at random from the distribution. In other words, examples don't influence each other. (An alternate explanation: i.i.d. is a way of referring to the randomness of variables.)\n",
    "- The distribution is **stationary**; that is the distribution doesn't change within the data set.\n",
    "- We draw examples from partitions from the **same distribution**.\n",
    "\n",
    "In practice, we sometimes violate these assumptions. For example:\n",
    "\n",
    "- Consider a model that chooses ads to display. The i.i.d. assumption would be violated if the model bases its choice of ads, in part, on what ads the user has previously seen.\n",
    "- Consider a data set that contains retail sales information for a year. User's purchases change seasonally, which would violate  stationarity.\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "# **SUMMARY - HYPERPARAMETER TRAINING**\n",
    "\n",
    "\n",
    "\n",
    "- Training loss should steadily decrease, steeply at first, and then more slowly until the slope of the curve reaches or approaches zero.\n",
    "- If the training loss does not converge, train for more epochs.\n",
    "- If the training loss decreases too slowly, increase the learning rate. Note that setting the learning rate too high may also prevent training loss from converging.\n",
    "- If the training loss varies wildly (that is, the training loss jumps around), decrease the learning rate.\n",
    "- Lowering the learning rate while increasing the number of epochs or the batch size is often a good combination.\n",
    "- Setting the batch size to a very small batch number can also cause instability. First, try large batch size values. Then, decrease the batch size until you see degradation.\n",
    "- For real-world datasets consisting of a very large number of examples, the entire dataset might not fit into memory. In such cases, you'll need to reduce the batch size to enable a batch to fit into memory.\n",
    "- Remember: the ideal combination of hyperparameters is data dependent, so you must always experiment and verify.\n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "# **TRAINING NEURAL NETWORKS - BEST PRACTICES**\n",
    "\n",
    "## Failure Cases\n",
    "There are a number of common ways for backpropagation to go wrong.\n",
    "\n",
    "### Vanishing Gradients\n",
    "The gradients for the lower layers (closer to the input) can become very small. In deep networks, computing these gradients can involve taking the product of many small terms.\n",
    "\n",
    "When the gradients vanish toward 0 for the lower layers, these layers train very slowly, or not at all.\n",
    "\n",
    "The ReLU activation function can help prevent vanishing gradients.\n",
    "\n",
    "### Exploding Gradients\n",
    "If the weights in a network are very large, then the gradients for the lower layers involve products of many large terms. In this case you can have exploding gradients: gradients that get too large to converge.\n",
    "\n",
    "Batch normalization can help prevent exploding gradients, as can lowering the learning rate.\n",
    "\n",
    "### Dead ReLU Units\n",
    "Once the weighted sum for a ReLU unit falls below 0, the ReLU unit can get stuck. It outputs 0 activation, contributing nothing to the network's output, and gradients can no longer flow through it during backpropagation. With a source of gradients cut off, the input to the ReLU may not ever change enough to bring the weighted sum back above 0.\n",
    "\n",
    "Lowering the learning rate can help keep ReLU units from dying.\n",
    "\n",
    "## Dropout Regularization\n",
    "Yet another form of regularization, called Dropout, is useful for neural networks. It works by randomly \"dropping out\" unit activations in a network for a single gradient step. The more you drop out, the stronger the regularization:\n",
    "\n",
    "0.0 = No dropout regularization.\n",
    "1.0 = Drop out everything. The model learns nothing.\n",
    "Values between 0.0 and 1.0 = More useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "\n",
    "## **MULTICLASS NEURAL NETWORK**\n",
    "\n",
    "**ONE VS ALL** is ok when the number of classes is small\n",
    "\n",
    "<img src=\"../img/one-vs-all.png\" width=\"60%\">\n",
    "\n",
    "\n",
    "**SOFTMAX** when using many labels; the sum of the probabilities in the output layer needs to be equal to 1.\n",
    "\n",
    "<img src=\"../img/softmax.png\" width=\"60%\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Full Softmax:  Softmax calculates a probability for every possible class.\n",
    "- Candidate sampling means that Softmax calculates a probability for all the positive labels but only for a random sample of negative labels. For example, if we are interested in determining whether an input image is a beagle or a bloodhound, we don't have to provide probabilities for every non-doggy example.\n",
    "\n",
    "Full Softmax is fairly cheap when the number of classes is small but becomes prohibitively expensive when the number of classes climbs. Candidate sampling can improve efficiency in problems having a large number of classes.\n",
    "\n",
    "**One Label vs. Many Labels**\n",
    "Softmax assumes that each example is a member of exactly one class. Some examples, however, can simultaneously be a member of multiple classes. For such examples:\n",
    "\n",
    "- You may not use Softmax.\n",
    "- You must rely on multiple logistic regressions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# **TRAINING AN EMBEDDING AS A PART OF A LARGER MODEL**\n",
    "\n",
    "<img src=\"../img/embeding.png\" width=\"60%\">"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
