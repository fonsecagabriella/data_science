{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of hyperparameter tuning\n",
    "\n",
    "\n",
    "\n",
    "- Training loss should steadily decrease, steeply at first, and then more slowly until the slope of the curve reaches or approaches zero.\n",
    "- If the training loss does not converge, train for more epochs.\n",
    "- If the training loss decreases too slowly, increase the learning rate. Note that setting the learning rate too high may also prevent training loss from converging.\n",
    "- If the training loss varies wildly (that is, the training loss jumps around), decrease the learning rate.\n",
    "- Lowering the learning rate while increasing the number of epochs or the batch size is often a good combination.\n",
    "- Setting the batch size to a very small batch number can also cause instability. First, try large batch size values. Then, decrease the batch size until you see degradation.\n",
    "- For real-world datasets consisting of a very large number of examples, the entire dataset might not fit into memory. In such cases, you'll need to reduce the batch size to enable a batch to fit into memory.\n",
    "- Remember: the ideal combination of hyperparameters is data dependent, so you must always experiment and verify.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# The ML fine print\n",
    "The following three basic assumptions guide generalization:\n",
    "\n",
    "- We draw examples **independently and identically** (i.i.d) at random from the distribution. In other words, examples don't influence each other. (An alternate explanation: i.i.d. is a way of referring to the randomness of variables.)\n",
    "- The distribution is **stationary**; that is the distribution doesn't change within the data set.\n",
    "- We draw examples from partitions from the **same distribution**.\n",
    "\n",
    "In practice, we sometimes violate these assumptions. For example:\n",
    "\n",
    "- Consider a model that chooses ads to display. The i.i.d. assumption would be violated if the model bases its choice of ads, in part, on what ads the user has previously seen.\n",
    "- Consider a data set that contains retail sales information for a year. User's purchases change seasonally, which would violate stationarity."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
