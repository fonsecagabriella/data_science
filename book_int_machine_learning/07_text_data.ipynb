{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with data\n",
    "\n",
    "_There is a helper function in scikit-learn to load files stores in a folder structure similar to the one below, where each subfolder corresponds to a label._\n",
    "\n",
    "<img src=\"../img/load_files_example.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of text_train: <class 'list'>\n",
      "Lenght of text_train 75000\n",
      "FIRST ENTRY \n",
      "\n",
      " b'Full of (then) unknown actors TSF is a great big cuddly romp of a film.<br /><br />The idea of a bunch of bored teenagers ripping off the local sink factory is odd enough, but add in the black humour that Forsyth & Co are so good at and your in for a real treat.<br /><br />The comatose van driver by itself worth seeing, and the canal side chase is just too real to be anything but funny.<br /><br />And for anyone who lived in Glasgow it\\'s a great \"Oh I know where that is\" film.'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "import numpy as np\n",
    "\n",
    "reviews_train = load_files(\"../data/aclImdb/train/\")\n",
    "\n",
    "# load_files returns a bunch, containing training texts and training labels\n",
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "\n",
    "print(f\"Type of text_train: {type(text_train)}\")\n",
    "print(f\"Lenght of text_train: {len(text_train)}\")\n",
    "print(f\"FIRST ENTRY \\n\\n {text_train[0]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the entry has some html markup. It is usually better to clean it up before proceeding with the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples per class (training): [12500 12500 50000]\n",
      "Number of documents in test data: 5\n",
      "Samples per class (Test): [12500 12500] \n"
     ]
    }
   ],
   "source": [
    "text_train = [doc.replace(b\"<br />\", b\" \") for doc in text_train]\n",
    "\n",
    "# check the number of samples per label\n",
    "print(f\"Samples per class (training): {np.bincount(y_train)}\")\n",
    "\n",
    "# loading the test set using the same approach as the one above\n",
    "\n",
    "reviews_test = load_files(\"../data/aclImdb/test/\")\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "\n",
    "print(f\"Number of documents in test data: {len(reviews_test)}\")\n",
    "print(f\"Samples per class (Test): {np.bincount(y_test)} \")\n",
    "\n",
    "text_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Representing text as a Bag of Words\n",
    "\n",
    "To do so, you mostly discart owrds that are not meaninful, such as stop-words. You basically count how many times each word appears in the document. Computing the bag of words consists of three steps:\n",
    "\n",
    "**01 / Tokenization**: Split each word by a delimitator, such as space and punctuation. Each word becomes now a *token*.\n",
    "\n",
    "**02 / Vocabulary building**: Collect all words that appear in any document, and then organise them (let's say, by alphabetic order)\n",
    "\n",
    "**03 / Enconding:** For each document, count how often a give word appears in the document.\n",
    "\n",
    "The bag-of-words representation is implemented in *CountVectorizer*, which is a transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 124255\n",
      "X_train: \n",
      " <75000x124255 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 10315542 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "vect = CountVectorizer().fit(text_train) #fits the vocabulary\n",
    "X_train = vect.transform(text_train) # transforms the data (encoding)\n",
    "\n",
    "print(f\"Vocabulary size: {len(vect.vocabulary_)}\")\n",
    "\n",
    "print(f\"X_train: \\n {repr(X_train)}\") # shows a representation of the sparse array used to store the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features 124255\n",
      "First 20 features \n",
      " ['00' '000' '0000' '0000000000000000000000000000000001' '0000000000001'\n",
      " '000000001' '000000003' '00000001' '000001745' '00001' '0001' '00015'\n",
      " '0002' '0007' '00083' '000ft' '000s' '000th' '001' '002']\n",
      "Features 150 to 200 \n",
      " ['10x' '10x20' '10yr' '10yrs' '11' '110' '1100' '11001001' '1100ad'\n",
      " '110mph' '110th' '111' '1119' '112' '11211' '1123745598' '112th' '113'\n",
      " '1130pm' '1138' '114' '1146' '115' '1150' '116' '1165' '116577' '117'\n",
      " '1172' '118' '1188' '118k' '119' '1192' '1193' '11f' '11in' '11m' '11pm'\n",
      " '11th' '12' '120' '1200' '1200f' '1201' '1202' '12076' '1209' '120mints'\n",
      " '120minutes']\n",
      "Every 2000th feature ['00' '_require_' 'aideed' 'announcement' 'asteroid' 'banquière'\n",
      " 'besieged' 'bollwood' 'btvs' 'carboni' 'chcialbym' 'clotheth'\n",
      " 'consecration' 'cringeful' 'deadness' 'devagan' 'doberman' 'duvall'\n",
      " 'endocrine' 'existent' 'fetiches' 'formatted' 'garard' 'godlie' 'gumshoe'\n",
      " 'heathen' 'honoré' 'immatured' 'interested' 'jewelry' 'kerchner' 'köln'\n",
      " 'leydon' 'lulu' 'mardjono' 'meistersinger' 'misspells' 'mumblecore'\n",
      " 'ngah' 'oedpius' 'overwhelmingly' 'penned' 'pleading' 'previlage'\n",
      " 'quashed' 'recreating' 'reverent' 'ruediger' 'sceme' 'settling'\n",
      " 'silveira' 'soderberghian' 'stagestruck' 'subprime' 'tabloids' 'themself'\n",
      " 'tpf' 'tyzack' 'unrestrained' 'videoed' 'weidler' 'worrisomely'\n",
      " 'zombified']\n"
     ]
    }
   ],
   "source": [
    "# another way to get more details about the vocabulary\n",
    "feature_names = vect.get_feature_names_out()\n",
    "\n",
    "print(f\"Number of features {len(feature_names)}\")\n",
    "print(f\"First 20 features \\n {feature_names[:20]}\")\n",
    "print(f\"Features 150 to 200 \\n {feature_names[150:200]}\")\n",
    "print(f\"Every 2000th feature {feature_names[::2000]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy 0.7005333333333332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# First simple model\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "scores = cross_val_score(LogisticRegression(solver='saga', max_iter=500), X_train, y_train, cv=5)\n",
    "print(f\"Mean cross-validation accuracy {np.mean(scores)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
