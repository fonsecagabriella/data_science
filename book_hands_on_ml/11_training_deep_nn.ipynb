{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Deep Neural Networks\n",
    "\n",
    "## Practical Guidelines\n",
    "\n",
    "<img src=\"../img/default_dnn.png\" width=\"30%\">\n",
    "\n",
    "**Additional consideratios**\n",
    "- Stadardize input features\n",
    "- Reuse parts of a pre-trained NN that solves a similar problem\n",
    "- Use unsupervised pretraining if you have a lot of unlabeled data\n",
    "- If your model self-normalizes: If it overfits the training set, then you should add alpha dropout (and always use early stopping as well). Do not use other regularization methods, or else they would break self-normalization.\n",
    "- If your model cannot self-normalize (e.g., it is a recurrent net or it contains skip connections):\n",
    "\n",
    "-- You can try using ELU (or another activation function) instead of SELU, it may perform better. Make sure to change the initialization method accord‐ ingly (e.g., He init for ELU or ReLU).\n",
    "\n",
    "-- If it is a deep network, you should use Batch Normalization after every hidden layer. If it overfits the training set, you can also try using max-norm or l2 reg‐ ularization.\n",
    "\n",
    "- If you need a sparse model, you can use l1 regularization (and optionally zero out the tiny weights after training). If you need an even sparser model, you can try using FTRL instead of Nadam optimization, along with l1 regularization. In any case, this will break self-normalization, so you will need to switch to BN if your model is deep.\n",
    "- If you need a low-latency model (one that performs lightning-fast predictions), you may need to use less layers, avoid Batch Normalization, and possibly replace the SELU activation function with the leaky ReLU. Having a sparse model will also help. You may also want to reduce the float precision from 32-bits to 16-bit (or even 8-bits)\n",
    "- If you are building a risk-sensitive application, or inference latency is not very important in your application, you can use MC Dropout to boost performance and get more reliable probability estimates, along with uncertainty estimates.\n",
    "\n",
    "\n",
    "\n",
    "**Vanishing / Exploding Gradients Problem:** \n",
    "When the gradients become too big / too small to be propagated through the NN.\n",
    "Solution: Use a BatchNormalisation after each layer, except the output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "        keras.layers.Flatten(input_shape=[28, 28]),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WAYS TO SPEED UP TRAINING**:\n",
    "\n",
    "- Good initialisation strategy for connection weights\n",
    "- Good activation function\n",
    "- Batch normalisation / Gradient Clipping\n",
    "- Reuse parts of a pre-trained network\n",
    "- Faster optimisers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function & Activation\n",
    "So which activation function should you use for the hidden layers of your deep neural networks? \n",
    "\n",
    "In general **SELU > ELU > leaky ReLU (and its variants) > ReLU > tanh > logistic.** \n",
    "- If the network’s architecture prevents it from self- normalizing, then ELU may perform better than SELU (since SELU is not smooth at z = 0). \n",
    "- If you care a lot about runtime latency, then you may prefer leaky ReLU. If you don’t want to tweak yet another hyperparameter, you may just use the default α values used by Keras (e.g., 0.3 for the leaky ReLU). \n",
    "- If you have spare time and computing power, you can use cross-validation to evaluate other activation functions, in particular RReLU if your network is over‐ fitting, or PReLU if you have a huge training set.\n",
    "\n",
    "- To use the leaky ReLU activation function, you must create a LeakyReLU instance like this:\n",
    "\n",
    "> leaky_relu = keras.layers.LeakyReLU(alpha=0.2)\n",
    "\n",
    "> layer = keras.layers.Dense(10, activation=leaky_relu, kernel_initializer=\"he_normal\")\n",
    "\n",
    "- For PReLU, just replace LeakyRelu(alpha=0.2) with PReLU().\n",
    "\n",
    "- For SELU activation, just set activation=\"selu\" and kernel_initial izer=\"lecun_normal\" when creating a layer:\n",
    "> layer = keras.layers.Dense(10, activation=\"selu\", kernel_initializer=\"lecun_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Clipping\n",
    "\n",
    "To solve the issue of exploding gradients in RNNs, it's better to use Gradient Clipping because the implementation of Batch Normalisation is more tricky.\n",
    "To do so, just use the clipvalue when creating an optimiser:\n",
    "\n",
    "> optimiser = keras.optimizer.SGD(clipvalue=1.0)  #clip every component of the gradient vector to a value between –1.0 and 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also clip the norm, instead of the value, if you notice the gradients explode (look for more info on how to do that)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster Optimizers\n",
    "\n",
    "**Momentum optimization:** you just have to set the parameter *momentum* (0.9 is usually a good value)\n",
    ">> optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9) \n",
    "\n",
    "**Nesterov Accelerated Gradient** sometimes improves performance. It's a variation of momentum optimization. To use set *nesterov=True*\n",
    ">> optmizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)\n",
    "\n",
    "**AdaGrad** this algorithm decays the learning rate, but it does it faster for steep dimensions than for dimensions with deeper slopes (known as *adaptative learning rate*).  It doesn't work well for NNs, but it's sufficient for Linear Regression.\n",
    "\n",
    "**RMSProp** it's a variation of AdaGrad, but only accumulates gradients of recent iterations, so it performs better. The default valud of rho works well.\n",
    "\n",
    ">> optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)\n",
    "\n",
    "**Adam** (adaptative moment estimation) it's a variation of Momentum optimisation and RMSProp. The default values work well because it's an adaptive learning algorithm.\n",
    "\n",
    ">> optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Schedules\n",
    "\n",
    "Instead of using a fixed learning rate, start with a high value and divide it by 3 until the algorithm stops diverging. \n",
    "\n",
    "Ps: You can set the learning schedule in the callback function. \n",
    "\n",
    "Other techniques:\n",
    "\n",
    "| Learning Rate Schedule  | Use Case                                         | Implementation Example                                                                                       |\n",
    "|--------------------------|-------------------------------------------------|-------------------------------------------------------------------------------------------------------------|\n",
    "| **Constant Learning Rate** | Default choice for many models. Use when unsure about dynamic adjustments. | `optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)`                                                |\n",
    "| **Time-based Decay**      | Reduce learning rate over time (e.g., large datasets). | `lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(initial_learning_rate=0.001, decay_steps=10000, decay_rate=0.5)`<br>`optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)` |\n",
    "| **Step Decay**            | Reduce learning rate at specific epochs.       | `lr_schedule = tf.keras.optimizers.schedules.PiecewiseConstantDecay([10000, 20000], [0.001, 0.0005, 0.0001])`<br>`optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)` |\n",
    "| **Exponential Decay**     | Gradually reduce the learning rate exponentially. | `lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.001, decay_rate=0.96, decay_steps=10000)`<br>`optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)` |\n",
    "| **Cosine Decay**          | Oscillate learning rate for cyclical patterns. | `lr_schedule = tf.keras.optimizers.schedules.CosineDecay(initial_learning_rate=0.001, decay_steps=10000)`<br>`optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)` |\n",
    "| **Cosine Decay with Warm Restarts** | Reset learning rate periodically, good for cyclical tasks. | `lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(initial_learning_rate=0.001, first_decay_steps=1000)`<br>`optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)` |\n",
    "| **Learning Rate Finder** | Find an optimal learning rate by experimenting. | Use `tf.keras.callbacks.LearningRateScheduler` with a custom function to adjust the learning rate dynamically. |\n",
    "| **Reduce on Plateau**    | Automatically reduce the learning rate when a metric (e.g., loss) stops improving. | `callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10)`<br>`model.fit(..., callbacks=[callback])` |\n",
    "\n",
    "---\n",
    "\n",
    "### Notes\n",
    "- Replace `tf.keras.optimizers.Adam` with any optimizer you're using.\n",
    "- Learning rate schedules help improve model performance and convergence.\n",
    "- Start simple (e.g., constant learning rate) and experiment with others as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoiding Overfitting Through Regularisation\n",
    "\n",
    "### L1 & L2 \n",
    "- You should apply reg to each layer\n",
    "- To avoid issues, you can use the second example from below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of l1 regularization\n",
    "layer = keras.layers.Dense(100, activation=\"elu\",\n",
    "                               kernel_initializer=\"he_normal\",\n",
    "                               kernel_regularizer=keras.regularizers.l2(0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of how you can implement regularisation in all layers\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "RegularizedDense = partial(keras.layers.Dense,\n",
    "                                activation=\"elu\",\n",
    "                                kernel_initializer=\"he_normal\",\n",
    "                                kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "         keras.layers.Flatten(input_shape=[28, 28]),\n",
    "         RegularizedDense(300),\n",
    "         RegularizedDense(100),\n",
    "         RegularizedDense(10, activation=\"softmax\",\n",
    "                          kernel_initializer=\"glorot_uniform\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "This is a popular technique. At every step, every neuron (excluding the output), have probability p of being dropped out. P is called the dropout rate, typically set at 50%.\n",
    "\n",
    "- Dropout is only applied during training. *So you can't compare validation and training loss*. Make sure to evaluate training loss after training in this case.\n",
    "\n",
    "- If you see the model is overfitting, you can increase dropout, and decrease if the other way around.\n",
    "\n",
    "- You can also implement dropout only after the last hidden layer (this is common). \n",
    "\n",
    "- It can slow the training, but the performance usually pays off. \n",
    "\n",
    "- If you want to regularize a self-normalizing network based on the SELU activation function, use AlphaDropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of dropout implementation\n",
    "model = keras.models.Sequential([\n",
    "         keras.layers.Flatten(input_shape=[28, 28]),\n",
    "         keras.layers.Dropout(rate=0.2),\n",
    "         keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "         keras.layers.Dropout(rate=0.2),\n",
    "         keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "         keras.layers.Dropout(rate=0.2),\n",
    "         keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte-Carlo Dropout\n",
    "\n",
    "It can be applied to a model without the need to retrain it.\n",
    "\n",
    "- The number of samples you use (100, int the example below), is a value you can tweak. If you set too many, it will take long, and the performance might not be too good.\n",
    "\n",
    "- What the code does is basically generating X predicitons for every instance in the test set, stacking them, and after averaging it. \n",
    "\n",
    "- If you're using Normalisation layers, you can't use the code as below (look for more information on how to do this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of Monte Carlo dropout\n",
    "\n",
    "with keras.backend.learning_phase_scope(1): # force training mode = dropout on\n",
    "    y_probas = np.stack([model.predict(X_test_scaled) for sample in range(100)])\n",
    "\n",
    "y_proba = y_probas.mean(axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max-Norm Regularisation\n",
    "\n",
    "To implement max-norm regularization in Keras, just set every hidden layer’s ker nel_constraint argument to a max_norm() constraint, with the appropriate max value, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\",\n",
    "                       kernel_constraint=keras.constraints.max_norm(1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTES\n",
    "\n",
    "- Glorot initialization and He initialization were designed to make the output standard deviation as close as possible to the input standard deviation, at least at the beginning of training. This reduces the vanishing/exploding gradients problem.\n",
    "\n",
    "- **All weights should be sampled independently; they should not all have the same initial value.** One important goal of sampling weights randomly is to break symmetry: if all the weights have the same initial value, even if that value is not zero, then symmetry is not broken (i.e., all neurons in a given layer are equivalent), and backpropagation will be unable to break it. Concretely, this means that all the neurons in any given layer will always have the same weights. It's like having just one neuron per layer, and much slower. It is virtually impossible for such a configuration to converge to a good solution.\n",
    "\n",
    "- **It is perfectly fine to initialize the bias terms to zero.** Some people like to initialize them just like weights, and that's OK too; it does not make much difference.\n",
    "\n",
    "- ReLU is usually a good default for the hidden layers, as it is fast and yields good results. Its ability to output precisely zero can also be useful in some cases (e.g., see Chapter 17). Moreover, it can sometimes benefit from optimized implementations as well as from hardware acceleration. \n",
    "\n",
    "- The leaky ReLU variants of ReLU can improve the model's quality without hindering its speed too much compared to ReLU. For large neural nets and more complex problems, GLU, Swish and Mish can give you a slightly higher quality model, but they have a computational cost. \n",
    "\n",
    "- The hyperbolic tangent (tanh) can be useful in the output layer if you need to output a number in a fixed range (by default between –1 and 1), but nowadays it is not used much in hidden layers, except in recurrent nets. \n",
    "\n",
    "- The sigmoid activation function is also useful in the output layer when you need to estimate a probability (e.g., for binary classification), but it is rarely used in hidden layers (there are exceptions—for example, for the coding layer of variational autoencoders;).\n",
    "\n",
    "- The softplus activation function is useful in the output layer when you need to ensure that the output will always be positive. \n",
    "\n",
    "- The softmax activation function is useful in the output layer to estimate probabilities for mutually exclusive classes, but it is rarely (if ever) used in hidden layers.\n",
    "\n",
    "- If you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer, then the algorithm will likely pick up a lot of speed, hopefully moving roughly toward the global minimum, but its momentum will carry it right past the minimum. Then it will slow down and come back, accelerate again, overshoot again, and so on. It may oscillate this way many times before converging, so overall it will take much longer to converge than with a smaller momentum value.\n",
    "\n",
    "- One way to produce a sparse model (i.e., with most weights equal to zero) is to train the model normally, then zero out tiny weights. For more sparsity, you can apply ℓ1 regularization during training, which pushes the optimizer toward sparsity. A third option is to use the TensorFlow Model Optimization Toolkit.\n",
    "\n",
    "- Dropout does slow down training, in general roughly by a factor of two. However, it has no impact on inference speed since it is only turned on during training. MC Dropout is exactly like dropout during training, but it is still active during inference, so each inference is slowed down slightly. More importantly, when using MC Dropout you generally want to run inference 10 times or more to get better predictions. This means that making predictions is slowed down by a factor of 10 or more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
