{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bdec2d3",
   "metadata": {},
   "source": [
    "# 🧠 Enconders - Summary\n",
    "- Use OneHotEncoder when you want full control, need to work with sklearn pipelines, or must handle unknown categories safely.\n",
    "\n",
    "- Use DictVectorizer when your data is in dictionary format (e.g., JSON or from APIs) and you want to plug it into a pipeline quickly.\n",
    "\n",
    "- Use pd.get_dummies() for quick, simple transformations when you're staying inside pandas and not building a full ML pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d5ccb0",
   "metadata": {},
   "source": [
    "# OneHotEncoder vs. DictVectorizer\n",
    "\n",
    "Two scikit‑learn transformers that both turn categorical data into numerical vectors — but they’re built around **different mental models of your data**.\n",
    "\n",
    "| Feature | **OneHotEncoder** | **DictVectorizer** |\n",
    "|---------|------------------|--------------------|\n",
    "| **Data model** | *Column‑oriented table* (NumPy array or pandas DataFrame). Each column has a fixed meaning. | *Bag‑of‑features* (list/iterator of Python dicts). Each `(key, value)` pair is an independent feature. |\n",
    "| **Typical raw input** | `pd.DataFrame` from CSV / SQL. | JSON‑like records, log entries, API payloads. |\n",
    "| **How categories are discovered** | Per column: learns unique values *within* each column. | Each distinct `(key, value)` becomes its own feature name `key=value`. |\n",
    "| **Numeric features** | Must be handled separately (e.g., via `ColumnTransformer`). | Numeric *values* pass through unchanged; only strings are one‑hot‑encoded. |\n",
    "| **Unknown categories at inference** | Controlled with `handle_unknown=('error', 'ignore', 'infrequent_if_exist')`. | New `(key, value)` raises an error unless pre‑declared. |\n",
    "| **Drop reference level** | Built‑in (`drop='first'`, `drop='if_binary'`). | Not built‑in; slice columns manually if needed. |\n",
    "| **Output format** | CSR sparse (default) or dense (`sparse_output=False`). | CSR sparse (default) or dense (`sparse=False`). |\n",
    "| **Inverse transform** | ✔️ Returns structured array/DataFrame. | ✔️ Returns list of dicts. |\n",
    "| **Best for** | Stable, tidy tabular data; fine‑grained per‑column control in pipelines. | Flexible, high‑dimensional, sparse feature spaces from dict/JSON inputs. |\n",
    "\n",
    "---\n",
    "\n",
    "## Deeper Intuition\n",
    "\n",
    "- **OneHotEncoder = “fixed schema, flexible values.”**  \n",
    "  Think spreadsheet columns (*Color*, *Size*, *City*). Encoder catalogs possible values **within** each column and builds block‑wise one‑hot columns.\n",
    "\n",
    "- **DictVectorizer = “flexible schema, key‑value explosion.”**  \n",
    "  Logging events: each line can have different keys (`device=Android`, `browser=Chrome`, `temperature=21.4`). Every new `(key, value)` pair spawns a feature; numeric values flow through unchanged.\n",
    "\n",
    "---\n",
    "\n",
    "## Minimal Example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62c4f7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneHotEncoder output:\n",
      " [[0. 1. 0. 1. 0.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 1.]]\n",
      "DictVectorizer output:\n",
      " [[0.  1.  0.  1.  0.  1.1]\n",
      " [1.  0.  1.  0.  0.  2. ]\n",
      " [0.  1.  0.  0.  1.  1.5]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"color\":  [\"red\", \"blue\", \"red\"],\n",
    "    \"size\":   [\"M\",   \"L\",    \"S\" ],\n",
    "    \"weight\": [1.1,    2.0,    1.5]\n",
    "})\n",
    "records = df.to_dict(orient=\"records\")    # [{'color':'red',...}, ...]\n",
    "\n",
    "# 1️⃣ OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse_output=False)\n",
    "X_ohe = ohe.fit_transform(df[[\"color\", \"size\"]])  # numeric handled elsewhere\n",
    "\n",
    "# 2️⃣ DictVectorizer\n",
    "dv = DictVectorizer(sparse=False)\n",
    "X_dv = dv.fit_transform(records)\n",
    "\n",
    "\n",
    "print(\"OneHotEncoder output:\\n\", X_ohe)\n",
    "print(\"DictVectorizer output:\\n\", X_dv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c344fd3a",
   "metadata": {},
   "source": [
    "### Resulting feature names\n",
    "\n",
    "| Encoder | Feature names |\n",
    "|---------|---------------|\n",
    "| **ohe** | `color=blue`, `color=red`, `size=L`, `size=M`, `size=S` |\n",
    "| **dv**  | `color=blue`, `color=red`, `size=L`, `size=M`, `size=S`, **`weight`** |\n",
    "\n",
    "`weight` automatically passes through with `DictVectorizer`.\n",
    "\n",
    "---\n",
    "\n",
    "### Quick Selection Guide\n",
    "\n",
    "1. **Tidy DataFrame → OneHotEncoder** (use in a `ColumnTransformer`).\n",
    "2. **List of dicts / JSON → DictVectorizer** (zero friction).\n",
    "3. **Need to drop reference level or handle unseen categories gracefully → OneHotEncoder**.\n",
    "4. **Just playing inside pandas, no pipeline → `pd.get_dummies()`** is quickest, though less production‑friendly.\n",
    "\n",
    "> **Rule of thumb:** choose the transformer whose **input format matches your raw data** and whose **options match your production constraints** (memory, unknown categories, multicollinearity).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
