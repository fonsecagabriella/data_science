{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change name of columns to lower case and replace spaces with underscores\n",
    "df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    "\n",
    "# change text of strig columns to lower case and replace spaces with underscores\n",
    "# TO IMPLEMENT\n",
    "strings = list(df.dtypes[df.dtypes == 'object'].index)\n",
    "for col in strings:\n",
    "    df[col] = df[col].str.lower().str.replace(' ', '_')\n",
    "\n",
    "# check the unique values in columns\n",
    "for col in df.columns:\n",
    "    print(col)\n",
    "    print(df[col].unique()[:5]) # print first 5 unique values\n",
    "    print(df[col].nunique()) # print number of unique values\n",
    "    print()\n",
    "\n",
    "\n",
    "# check missing values\n",
    "\n",
    "# check target variable distribution\n",
    "# apply log if the target variable is skewed\n",
    "# do not forget to treat log 0 as adding 1, since log 0  is undefined\n",
    "price_logs = np.log1p(df.msrp) # you can apply log1p to treat log 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check relationship between variables\n",
    "\n",
    "- **FOR CATEGORICAL** Use mutual_info_score\n",
    "- **FOR NUMERICAL** Use pearson (note: only linear relationships) / Strong if r>0.6, moderate between 0.2 & 0.5\n",
    "\n",
    "| Feature               | Mutual Information (MI)        | Correlation (e.g., Pearson)   |\n",
    "|-----------------------|---------------------------------|--------------------------------|\n",
    "| **Type of Relationship** | Measures **any** kind of dependency (linear & nonlinear) | Measures **only linear** relationships |\n",
    "| **Range of Values**   | Always **≥ 0** (0 = no relation, higher = stronger dependency) | Between **-1 and 1** (-1 = strong negative, 1 = strong positive, 0 = no correlation) |\n",
    "| **Interpretation**    | Measures **how much knowing one variable reduces uncertainty** about the other | Measures **how much one variable changes proportionally** with another |\n",
    "| **Handles Categorical Data?** | ✅ Yes (works for both categorical & numerical data) | ❌ No (only works well for numerical data) |\n",
    "| **Sensitivity to Scale** | Not affected by scaling | Strongly affected by scaling (needs normalization) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_quality(df):\n",
    "    # Store initial data quality metrics\n",
    "    quality_report = {\n",
    "        'missing_values': df.isnull().sum().to_dict(),\n",
    "        'duplicates': df.duplicated().sum(),\n",
    "        'total_rows': len(df),\n",
    "        'memory_usage': df.memory_usage().sum() / 1024**2  # in MB\n",
    "    }\n",
    "    return quality_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardise data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardise_datatypes(df):\n",
    "    for column in df.columns:\n",
    "        # Try converting string dates to datetime\n",
    "        if df[column].dtype == 'object':\n",
    "            try:\n",
    "                df[column] = pd.to_datetime(df[column])\n",
    "                print(f\"Converted {column} to datetime\")\n",
    "            except ValueError:\n",
    "                # Try converting to numeric if datetime fails\n",
    "                try:\n",
    "                    df[column] = pd.to_numeric(df[column].str.replace('$', '').str.replace(',', ''))\n",
    "                    print(f\"Converted {column} to numeric\")\n",
    "                except:\n",
    "                    pass\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def handle_missing_values(df):\n",
    "    # Handle numeric columns\n",
    "    numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    if len(numeric_columns) > 0:\n",
    "        num_imputer = SimpleImputer(strategy='median')\n",
    "        df[numeric_columns] = num_imputer.fit_transform(df[numeric_columns])\n",
    "    \n",
    "    # Handle categorical columns\n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "    if len(categorical_columns) > 0:\n",
    "        cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "        df[categorical_columns] = cat_imputer.fit_transform(df[categorical_columns])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df):\n",
    "    numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    outliers_removed = {}\n",
    "\n",
    "    for column in numeric_columns:\n",
    "        Q1 = df[column].quantile(0.25)\n",
    "        Q3 = df[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        # Count outliers before removing\n",
    "        outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)].shape[0]\n",
    "\n",
    "        # Cap the values instead of removing them\n",
    "        df[column] = df[column].clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "        if outliers > 0:\n",
    "            outliers_removed[column] = outliers\n",
    "\n",
    "    return df, outliers_removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_cleaning(df, original_shape, cleaning_report):\n",
    "    validation_results = {\n",
    "        'rows_remaining': len(df),\n",
    "        'missing_values_remaining': df.isnull().sum().sum(),\n",
    "        'duplicates_remaining': df.duplicated().sum(),\n",
    "        'data_loss_percentage': (1 - len(df)/original_shape[0]) * 100\n",
    "    }\n",
    "    \n",
    "    # Add validation results to the cleaning report\n",
    "    cleaning_report['validation'] = validation_results\n",
    "    return cleaning_report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
