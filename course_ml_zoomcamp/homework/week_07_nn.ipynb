{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5a94ca0",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "In this homework, we'll build a model for classifying various hair types. \n",
    "For this, we will use the Hair Type dataset that was obtained from \n",
    "[Kaggle](https://www.kaggle.com/datasets/kavyasreeb/hair-type-dataset) \n",
    "and slightly rebuilt.\n",
    "\n",
    "You can download the target dataset for this homework from \n",
    "[here](https://github.com/SVizor42/ML_Zoomcamp/releases/download/straight-curly-data/data.zip):\n",
    "\n",
    "```bash\n",
    "wget https://github.com/SVizor42/ML_Zoomcamp/releases/download/straight-curly-data/data.zip\n",
    "unzip data.zip\n",
    "```\n",
    "The dataset is split into train and test dataset. Use train dataset to train the model and test dataset for validation. \n",
    "\n",
    "In the lectures we saw how to use a pre-trained neural network. In the homework, we'll train a much smaller model from scratch. \n",
    "\n",
    "We will use PyTorch for that.\n",
    "\n",
    "You can use Google Colab or your own computer for that.\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "The dataset contains around 1000 images of hairs in the separate folders \n",
    "for training and test sets. \n",
    "\n",
    "### Reproducibility\n",
    "\n",
    "Reproducibility in deep learning is a multifaceted challenge that requires attention \n",
    "to both software and hardware details. In some cases, we can't guarantee exactly the same results during the same experiment runs.\n",
    "\n",
    "Therefore, in this homework we suggest to set the random number seed generators by:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "```\n",
    "\n",
    "Also, use PyTorch of version 2.8.0 (that's the one in Colab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8cc92ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b8d959",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "For this homework we will use Convolutional Neural Network (CNN). We'll use PyTorch.\n",
    "\n",
    "You need to develop the model with following structure:\n",
    "\n",
    "* The shape for input should be `(3, 200, 200)` (channels first format in PyTorch)\n",
    "* Next, create a convolutional layer (`nn.Conv2d`):\n",
    "    * Use 32 filters (output channels)\n",
    "    * Kernel size should be `(3, 3)` (that's the size of the filter), padding = 0, stride = 1\n",
    "    * Use `'relu'` as activation \n",
    "* Reduce the size of the feature map with max pooling (`nn.MaxPool2d`)\n",
    "    * Set the pooling size to `(2, 2)`\n",
    "* Turn the multi-dimensional result into vectors using `flatten` or `view`\n",
    "* Next, add a `nn.Linear` layer with 64 neurons and `'relu'` activation\n",
    "* Finally, create the `nn.Linear` layer with 1 neuron - this will be the output\n",
    "    * The output layer should have an activation - use the appropriate activation for the binary classification case\n",
    "\n",
    "As optimizer use `torch.optim.SGD` with the following parameters:\n",
    "\n",
    "* `torch.optim.SGD(model.parameters(), lr=0.002, momentum=0.8)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66331460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class HairTypeCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HairTypeCNN, self).__init__()\n",
    "\n",
    "        # 1) CONVOLUTION LAYER: in_channels=3, out_channels=32, kernel_size=3\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        #   \"Add a convolution layer with 32 filters, kernel size 3,\n",
    "        #    stride 1, padding 0\"\n",
    "        #\n",
    "        # In TensorFlow:\n",
    "        #   tf.keras.layers.Conv2D(32, 3, strides=1, padding=\"valid\")\n",
    "        #\n",
    "        # In PyTorch:\n",
    "        #   nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        #\n",
    "        # Input shape:  (3, 200, 200)\n",
    "        # Output shape: (32, 198, 198)\n",
    "        # ---------------------------------------------------------\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3,\n",
    "            out_channels=32,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=0\n",
    "        )\n",
    "\n",
    "        # 2) MAX-POOL LAYER 2x2\n",
    "        # In TensorFlow:\n",
    "        #   tf.keras.layers.MaxPooling2D(pool_size=2)\n",
    "        #\n",
    "        # In PyTorch:\n",
    "        #   nn.MaxPool2d(kernel_size, stride)\n",
    "        #\n",
    "        # Conv output: (32, 198, 198)\n",
    "        # After pooling: (32, 99, 99)\n",
    "        # ------------------------------\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # After conv + pool:\n",
    "        # Input:  (3, 200, 200)\n",
    "        # Conv:   (32, 198, 198)\n",
    "        # Pool:   (32, 99, 99)\n",
    "\n",
    "        # 3) COMPUTE FLATTEN SIZE\n",
    "        # After Conv â†’ Pool we know the tensor is:\n",
    "        #   (batch, 32, 99, 99)\n",
    "        # Flatten size = 32 * 99 * 99 = 313632\n",
    "        self.flatten_size = 32 * 99 * 99\n",
    "\n",
    "        # 4) FIRST FULLY CONNECTED LAYER (dense layer)\n",
    "        #   \"Add a fully-connected layer with 64 units\"\n",
    "        #\n",
    "        # In TensorFlow:\n",
    "        #   tf.keras.layers.Dense(64, activation=\"relu\")\n",
    "        #\n",
    "        # PyTorch separates Linear + activation\n",
    "        # ----------------------------------------\n",
    "        self.fc1 = nn.Linear(self.flatten_size, 64)\n",
    "\n",
    "        # 5) OUTPUT LAYER\n",
    "        #   \"Add the output layer with 1 neuron\"\n",
    "        #\n",
    "        # Binary classification â†’ output logits\n",
    "        # (Note: use BCEWithLogitsLoss later)\n",
    "        #\n",
    "        # In TensorFlow:\n",
    "        #   Dense(1, activation=\"sigmoid\")\n",
    "        #\n",
    "        # In PyTorch:\n",
    "        #   Linear() gives raw logits\n",
    "        #   Sigmoid is applied inside the loss function\n",
    "        self.fc2 = nn.Linear(64, 1)  # 1 output neuron for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        # INPUT SHAPE: (batch, 3, 200, 200)\n",
    "        # x: (batch_size, 3, 200, 200)\n",
    "\n",
    "        # 1) Convolution\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        # Activation (PyTorch requires explicit activation)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # 2) Max Pooling\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # 3) Flatten (TensorFlow does this automatically)\n",
    "        x = x.view(x.size(0), -1)  # (batch_size, 313632)\n",
    "\n",
    "        # 4) Dense layer + activation\n",
    "        # Fully connected\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # 5) Output layer\n",
    "        x = self.fc2(x)  # logits, no sigmoid here\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "934b7a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model and move it to the appropriate device\n",
    "model = HairTypeCNN().to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7722df8c",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Which loss function you will use?\n",
    "\n",
    "> `nn.BCEWithLogitsLoss()`\n",
    "\n",
    "**ANSWER ðŸ‘©ðŸ½â€ðŸ’»**\n",
    "\n",
    "- I am doing binary classification with a single output neuron.\n",
    "- My model returns raw logits (no sigmoid inside the model).\n",
    "- `nn.BCEWithLogitsLoss()` is the correct loss for this setup because:\n",
    "    - It expects one output per sample (logit), exactly like my model produces.\n",
    "    - It internally applies a sigmoid + binary cross-entropy in a numerically stable way.\n",
    "    - It is the PyTorch equivalent of using `Dense(1, activation=\"sigmoid\")` + `BinaryCrossentropy` in TensorFlow.\n",
    "\n",
    "The other options don't match the task:\n",
    "\n",
    "- MSELoss treats the problem as regression â†’ not ideal.\n",
    "- CrossEntropyLoss requires two output classes, not a single logit.\n",
    "- CosineEmbeddingLoss is for similarity learning, not classification.\n",
    "\n",
    "So, `nn.BCEWithLogitsLoss()` is the correct and most stable loss for binary classification with a single-logit output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86c4ffa",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "What's the total number of parameters of the model? You can use `torchsummary` or count manually. \n",
    "\n",
    "In PyTorch, you can find the total number of parameters using:\n",
    "\n",
    "```python\n",
    "# Option 1: Using torchsummary (install with: pip install torchsummary)\n",
    "from torchsummary import summary\n",
    "summary(model, input_size=(3, 200, 200))\n",
    "\n",
    "# Option 2: Manual counting\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "```\n",
    "\n",
    "> 20073473"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "255b2e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 20073473\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73297c22",
   "metadata": {},
   "source": [
    "### Generators and Training\n",
    "\n",
    "For the next two questions, use the following transformation for both train and test sets.\n",
    "\n",
    "* We don't need to do any additional pre-processing for the images.\n",
    "* Use `batch_size=20`\n",
    "* Use `shuffle=True` for both training, but `False` for test. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d62a50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((200, 200)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ) # ImageNet normalization\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "380c9a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_transforms = transforms.Compose([\n",
    "    transforms.Resize((200, 200)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d41a47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets\n",
    "\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    root='data/train',\n",
    "    transform=train_transforms\n",
    ")\n",
    "\n",
    "validation_dataset = datasets.ImageFolder(\n",
    "    root='data/test',\n",
    "    transform=validation_transforms\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "770e3792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_loader = DataLoader(\n",
    "    validation_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a655b2ef",
   "metadata": {},
   "source": [
    "Now fit the model.\n",
    "\n",
    "You can use this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11d1de41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.5892, Acc: 0.6700, Val Loss: 0.6248, Val Acc: 0.6468\n",
      "Epoch 2/10, Loss: 0.5431, Acc: 0.7050, Val Loss: 0.6500, Val Acc: 0.6219\n",
      "Epoch 3/10, Loss: 0.5367, Acc: 0.7075, Val Loss: 0.6211, Val Acc: 0.6468\n",
      "Epoch 4/10, Loss: 0.4570, Acc: 0.7812, Val Loss: 0.6164, Val Acc: 0.6517\n",
      "Epoch 5/10, Loss: 0.3920, Acc: 0.8250, Val Loss: 0.6226, Val Acc: 0.6567\n",
      "Epoch 6/10, Loss: 0.4134, Acc: 0.8125, Val Loss: 0.6186, Val Acc: 0.7065\n",
      "Epoch 7/10, Loss: 0.3029, Acc: 0.8750, Val Loss: 0.7676, Val Acc: 0.6667\n",
      "Epoch 8/10, Loss: 0.2699, Acc: 0.9000, Val Loss: 0.8655, Val Acc: 0.6368\n",
      "Epoch 9/10, Loss: 0.2513, Acc: 0.8925, Val Loss: 0.6839, Val Acc: 0.7363\n",
      "Epoch 10/10, Loss: 0.2078, Acc: 0.9137, Val Loss: 0.8861, Val Acc: 0.6766\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "history = {'acc': [], 'loss': [], 'val_acc': [], 'val_loss': []}\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.002, momentum=0.8)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        labels = labels.float().unsqueeze(1) # Ensure labels are float and have shape (batch_size, 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        # For binary classification with BCEWithLogitsLoss, apply sigmoid to outputs before thresholding for accuracy\n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    epoch_acc = correct_train / total_train\n",
    "    history['loss'].append(epoch_loss)\n",
    "    history['acc'].append(epoch_acc)\n",
    "\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in validation_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            labels = labels.float().unsqueeze(1)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_running_loss += loss.item() * images.size(0)\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "    val_epoch_loss = val_running_loss / len(validation_dataset)\n",
    "    val_epoch_acc = correct_val / total_val\n",
    "    history['val_loss'].append(val_epoch_loss)\n",
    "    history['val_acc'].append(val_epoch_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "          f\"Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}, \"\n",
    "          f\"Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d2acd8",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "What is the median of training accuracy for all the epochs for this model?\n",
    "\n",
    "> 0.84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b6f3f5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median training accuracy over all epochs: 0.8187\n"
     ]
    }
   ],
   "source": [
    "# median of training accuracy\n",
    "median_acc = np.median(history['acc'])\n",
    "print(f\"Median training accuracy over all epochs: {median_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b577e7ae",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "What is the standard deviation of training loss for all the epochs for this model?\n",
    "\n",
    "> 0.171\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "195c6f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard deviation of training loss over all epochs: 0.1280\n"
     ]
    }
   ],
   "source": [
    "# std for training loss\n",
    "std_loss = np.std(history['loss'])\n",
    "print(f\"Standard deviation of training loss over all epochs: {std_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f0224b",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "For the next two questions, we'll generate more data using data augmentations. \n",
    "\n",
    "Add the following augmentations to your training data generator:\n",
    "\n",
    "```python\n",
    "transforms.RandomRotation(50),\n",
    "transforms.RandomResizedCrop(200, scale=(0.9, 1.0), ratio=(0.9, 1.1)),\n",
    "transforms.RandomHorizontalFlip(),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "78e98cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation for training data\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomRotation(50),\n",
    "    transforms.RandomResizedCrop(\n",
    "        size=200,\n",
    "        scale=(0.9, 1.0),\n",
    "        ratio=(0.9, 1.1),\n",
    "    ),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "    ),\n",
    "])\n",
    "\n",
    "# rebuild train dataset and loader with augmentation\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    root=\"data/train\",\n",
    "    transform=train_transforms\n",
    ")\n",
    "\n",
    "validation_dataset = datasets.ImageFolder(\n",
    "    root=\"data/test\",\n",
    "    transform=test_transforms\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=20,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=20,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01da5862",
   "metadata": {},
   "source": [
    "### Question 5 \n",
    "\n",
    "Let's train our model for 10 more epochs using the same code as previously.\n",
    "\n",
    "> **Note:** make sure you don't re-create the model.\n",
    "> we want to continue training the model we already started training.\n",
    "\n",
    "What is the mean of test loss for all the epochs for the model trained with augmentations?\n",
    "\n",
    "> 0.88\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "40597143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.6816, Train Acc: 0.6212, Test Loss: 0.6085, Test Acc: 0.6816\n",
      "Epoch 2/10, Train Loss: 0.5967, Train Acc: 0.6800, Test Loss: 0.6751, Test Acc: 0.6169\n",
      "Epoch 3/10, Train Loss: 0.5923, Train Acc: 0.6550, Test Loss: 0.6860, Test Acc: 0.6567\n",
      "Epoch 4/10, Train Loss: 0.5884, Train Acc: 0.6825, Test Loss: 0.6186, Test Acc: 0.6418\n",
      "Epoch 5/10, Train Loss: 0.5860, Train Acc: 0.6813, Test Loss: 0.6601, Test Acc: 0.6468\n",
      "Epoch 6/10, Train Loss: 0.5748, Train Acc: 0.7063, Test Loss: 0.6336, Test Acc: 0.6766\n",
      "Epoch 7/10, Train Loss: 0.5607, Train Acc: 0.7087, Test Loss: 0.6214, Test Acc: 0.6766\n",
      "Epoch 8/10, Train Loss: 0.5463, Train Acc: 0.7100, Test Loss: 0.6191, Test Acc: 0.6816\n",
      "Epoch 9/10, Train Loss: 0.5161, Train Acc: 0.7412, Test Loss: 0.6259, Test Acc: 0.7015\n",
      "Epoch 10/10, Train Loss: 0.5272, Train Acc: 0.7375, Test Loss: 0.6052, Test Acc: 0.6915\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6353473142604923"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_epochs_aug = 10\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(num_epochs_aug):\n",
    "    # ----- TRAIN -----\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for images, labels in train_loader:  # now with augmentation\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        labels = labels.float().unsqueeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    epoch_acc = correct_train / total_train\n",
    "\n",
    "    # ----- TEST -----\n",
    "    model.eval()\n",
    "    test_running_loss = 0.0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            labels = labels.float().unsqueeze(1)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            test_running_loss += loss.item() * images.size(0)\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "    test_epoch_loss = test_running_loss / len(test_dataset)\n",
    "    test_epoch_acc = correct_test / total_test\n",
    "    test_losses.append(test_epoch_loss)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{num_epochs_aug}, \"\n",
    "        f\"Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}, \"\n",
    "        f\"Test Loss: {test_epoch_loss:.4f}, Test Acc: {test_epoch_acc:.4f}\"\n",
    "    )\n",
    "\n",
    "# ðŸ‘‰ For Question 5:\n",
    "mean_test_loss = np.mean(test_losses)\n",
    "mean_test_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb543afe",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "What's the average of test accuracy for the last 5 epochs (from 6 to 10)\n",
    "for the model trained with augmentations?\n",
    "> 0.68\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "681f86e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test accuracy for last 5 epochs: 0.6846\n"
     ]
    }
   ],
   "source": [
    "# Average test accuracy for last 5 epochs\n",
    "avg_test_acc_last5 = np.mean(history['val_acc'][-5:])\n",
    "print(f\"Average test accuracy for last 5 epochs: {avg_test_acc_last5:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
